# Maryland OLP AI Chat Backend Environment Variables

# ============================================
# LLM Mode Configuration
# ============================================
# Options: faq, ollama, llamacpp, api, hybrid
# - faq: Only use pre-indexed FAQ (no AI)
# - ollama: Use local Ollama server
# - llamacpp: Use local llama.cpp server
# - api: Use cloud APIs (OpenRouter/Anthropic/OpenAI)
# - hybrid: Try local first, fallback to API
LLM_MODE=faq

# ============================================
# Local LLM Settings (Ollama)
# ============================================
OLLAMA_URL=http://localhost:11434
OLLAMA_MODEL=gpt-oss:20b
# Other good models: qwen3:30b, qwen3:8b, llama3.1:8b

# ============================================
# Local LLM Settings (llama.cpp)
# ============================================
LLAMACPP_URL=http://localhost:8080

# ============================================
# API Settings (for api/hybrid modes)
# ============================================

# OpenRouter (cheapest - DeepSeek R1 is $0.03/1M tokens!)
# Get key at: https://openrouter.ai/keys
OPENROUTER_API_KEY=
OPENROUTER_MODEL=deepseek/deepseek-r1-distill-llama-8b

# Anthropic Claude (optional fallback)
# Get key at: https://console.anthropic.com/
ANTHROPIC_API_KEY=

# OpenAI (optional fallback)
# Get key at: https://platform.openai.com/api-keys
OPENAI_API_KEY=

# ============================================
# Server Settings
# ============================================
PORT=5000
DEBUG=True
